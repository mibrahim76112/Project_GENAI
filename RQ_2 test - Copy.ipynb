{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Okl2JnUUhr"
      },
      "source": [
        "# Vector-Quantized Variational Autoencoders\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/07/21<br>\n",
        "**Last modified:** 2022/06/27<br>\n",
        "**Description:** Training a VQ-VAE for image reconstruction and codebook sampling for generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv71MA0DUUhv"
      },
      "source": [
        "In this example, we develop a Vector Quantized Variational Autoencoder (VQ-VAE).\n",
        "VQ-VAE was proposed in\n",
        "[Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)\n",
        "by van der Oord et al. In standard VAEs, the latent space is continuous and is sampled\n",
        "from a Gaussian distribution. It is generally harder to learn such a continuous\n",
        "distribution via gradient descent. VQ-VAEs, on the other hand,\n",
        "operate on a discrete latent space, making the optimization problem simpler. It does so\n",
        "by maintaining a discrete *codebook*. The codebook is developed by\n",
        "discretizing the distance between continuous embeddings and the encoded\n",
        "outputs. These discrete code words are then fed to the decoder, which is trained\n",
        "to generate reconstructed samples.\n",
        "\n",
        "For an overview of VQ-VAEs, please refer to the original paper and\n",
        "[this video explanation](https://www.youtube.com/watch?v=VZFVUrYcig0).\n",
        "If you need a refresher on VAEs, you can refer to\n",
        "[this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-12/).\n",
        "VQ-VAEs are one of the main recipes behind [DALL-E](https://openai.com/blog/dall-e/)\n",
        "and the idea of a codebook is used in [VQ-GANs](https://arxiv.org/abs/2012.09841).\n",
        "\n",
        "This example uses implementation details from the\n",
        "[official VQ-VAE tutorial](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb)\n",
        "from DeepMind.\n",
        "## Requirements\n",
        "To run this example, you will need TensorFlow 2.5 or higher, as well as\n",
        "TensorFlow Probability, which can be installed using the command below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3ZUN7rUUhy"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-hu-GqEYUUhy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLkzFH5UUUhy"
      },
      "source": [
        "## Residual VectorQuantizer layer\n",
        "\n",
        "First, we implement a custom layer for the vector quantizer, which is the layer in between\n",
        "the encoder and decoder. Consider an output from the encoder, with shape `(batch_size, height, width,\n",
        "num_filters)`. The vector quantizer will first flatten this output, only keeping the\n",
        "`num_filters` dimension intact. So, the shape would become `(batch_size * height * width,\n",
        "num_filters)`. The rationale behind this is to treat the total number of filters as the size for\n",
        "the latent embeddings.\n",
        "\n",
        "An embedding table is then initialized to learn a codebook. We measure the L2-normalized\n",
        "distance between the flattened encoder outputs and code words of this codebook. We take the\n",
        "code that yields the minimum distance, and we apply one-hot encoding to achieve quantization.\n",
        "This way, the code yielding the minimum distance to the corresponding encoder output is\n",
        "mapped as one and the remaining codes are mapped as zeros.\n",
        "\n",
        "Since the quantization process is not differentiable, we apply a\n",
        "[straight-through estimator](https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html)\n",
        "in between the decoder and the encoder, so that the decoder gradients are directly propagated\n",
        "to the encoder. As the encoder and decoder share the same channel space, the decoder gradients are\n",
        "still meaningful to the encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Q3wR9iUUh0"
      },
      "source": [
        "**A note on straight-through estimation**:\n",
        "\n",
        "This line of code does the straight-through estimation part: `quantized = x +\n",
        "tf.stop_gradient(quantized - x)`. During backpropagation, `(quantized - x)` won't be\n",
        "included in the computation graph and the gradients obtained for `quantized`\n",
        "will be copied for `inputs`. Thanks to [this video](https://youtu.be/VZFVUrYcig0?t=1393)\n",
        "for helping me understand this technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residual "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualVectorQuantizer(layers.Layer):\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_quantizers, beta=0.25, ema_decay=0.99, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.num_quantizers = num_quantizers  # Number of residual quantization stages\n",
        "        self.beta = beta\n",
        "        self.ema_decay = ema_decay  # EMA smoothing factor\n",
        "\n",
        "        # Create multiple embedding matrices (one per quantization stage)\n",
        "        w_init =  tf.keras.initializers.RandomUniform()\n",
        "        self.embeddings = [\n",
        "            tf.Variable(\n",
        "                initial_value=w_init(\n",
        "                    shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
        "                ),\n",
        "                trainable=True,\n",
        "                name=f\"embeddings_vqvae_{i}\",\n",
        "            )\n",
        "            for i in range(self.num_quantizers)\n",
        "        ]\n",
        "        \n",
        "        # EMA variables for each quantizer's embeddings\n",
        "        self.ema_embeddings = [\n",
        "            tf.Variable(\n",
        "                initial_value=tf.zeros_like(self.embeddings[i]),  # Initial EMA value is zero\n",
        "                trainable=False,\n",
        "                name=f\"ema_embeddings_vqvae_{i}\",\n",
        "            )\n",
        "            for i in range(self.num_quantizers)\n",
        "        ]\n",
        "\n",
        "    def call(self, x):\n",
        "        shape = tf.shape(x)  # Get dynamic shape\n",
        "        batch_size, height, width, channels = shape[0], shape[1], shape[2], shape[3]\n",
        "        flattened = tf.reshape(x, [batch_size * height * width, channels])\n",
        "\n",
        "        residual = flattened\n",
        "        quantized_output = tf.zeros_like(flattened)\n",
        "        total_commitment_loss = 0\n",
        "        total_codebook_loss = 0\n",
        "\n",
        "        for i in range(self.num_quantizers):\n",
        "            # Quantization for the current stage\n",
        "            encoding_indices = self.get_code_indices(residual, self.embeddings[i])\n",
        "            encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
        "            quantized = tf.matmul(encodings, self.embeddings[i], transpose_b=True)\n",
        "\n",
        "            # Compute loss\n",
        "            codebook_loss = tf.reduce_mean((tf.stop_gradient(residual) - quantized) ** 2)\n",
        "            commitment_loss = tf.reduce_mean((residual - tf.stop_gradient(quantized)) ** 2)\n",
        "            total_commitment_loss += commitment_loss\n",
        "            total_codebook_loss += codebook_loss\n",
        "\n",
        "            # Compute residual for the next stage\n",
        "            residual -= quantized\n",
        "            quantized_output += quantized\n",
        "\n",
        "            # Update the EMA embeddings using exponential decay\n",
        "            self.ema_embeddings[i].assign(\n",
        "                self.ema_decay * self.ema_embeddings[i] + (1 - self.ema_decay) * self.embeddings[i]\n",
        "            )\n",
        "\n",
        "        total_loss = self.beta * total_commitment_loss + total_codebook_loss\n",
        "        # Add total loss\n",
        "        self.add_loss(total_loss)\n",
        "\n",
        "        # Reshape back to (batch, height, width, channels)\n",
        "        quantized_output = tf.reshape(quantized_output, [batch_size, height, width, channels])\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized_output = x + tf.stop_gradient(quantized_output - x)\n",
        "        return quantized_output\n",
        "\n",
        "    def get_code_indices(self, inputs, embedding_matrix):\n",
        "        # Calculate L2-normalized distance between inputs and codebook vectors\n",
        "        similarity = tf.matmul(inputs, embedding_matrix)\n",
        "        distances = (\n",
        "            tf.reduce_sum(inputs ** 2, axis=1, keepdims=True)\n",
        "            + tf.reduce_sum(embedding_matrix ** 2, axis=0)\n",
        "            - 2 * similarity\n",
        "        )\n",
        "\n",
        "        # Get the index of the nearest codebook vector\n",
        "        encoding_indices = tf.argmin(distances, axis=1)\n",
        "        return encoding_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-aVrzU_UUh1"
      },
      "source": [
        "## Encoder and decoder\n",
        "\n",
        "Now for the encoder and the decoder for the VQ-VAE. We will keep them small so\n",
        "that their capacity is a good fit for the MNIST dataset. The implementation of the encoder and\n",
        "come from\n",
        "[this example](https://keras.io/examples/generative/vae).\n",
        "\n",
        "Note that activations _other than ReLU_ may not work for the encoder and decoder layers in the\n",
        "quantization architecture: Leaky ReLU activated layers, for example, have proven difficult to\n",
        "train, resulting in intermittent loss spikes that the model has trouble recovering from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JujsUdT2UUh1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_encoder(latent_dim=16):\n",
        "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
        "        encoder_inputs\n",
        "    )\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
        "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
        "\n",
        "\n",
        "def get_decoder(latent_dim=16):\n",
        "    latent_inputs = keras.Input(shape=get_encoder(latent_dim).output.shape[1:])\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
        "        latent_inputs\n",
        "    )\n",
        "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    decoder_outputs = layers.Conv2DTranspose(1, 3, padding=\"same\")(x)\n",
        "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining RQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"rvq_vae\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"rvq_vae\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vector_quantizer                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualVectorQuantizer</span>)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,033</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_47 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │        \u001b[38;5;34m19,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vector_quantizer                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mResidualVectorQuantizer\u001b[0m)       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │        \u001b[38;5;34m28,033\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,889</span> (187.07 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,889\u001b[0m (187.07 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,889</span> (187.07 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,889\u001b[0m (187.07 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "def get_vqvae(latent_dim=16, num_embeddings=64,num_quantizers =2):\n",
        "    rvq_layer = ResidualVectorQuantizer(num_embeddings, latent_dim,num_quantizers, name=\"vector_quantizer\")\n",
        "    encoder = get_encoder(latent_dim)\n",
        "    decoder = get_decoder(latent_dim)\n",
        "    inputs = keras.Input(shape=(28, 28, 1))\n",
        "    encoder_outputs = encoder(inputs)\n",
        "    quantized_latents = rvq_layer(encoder_outputs)\n",
        "    reconstructions = decoder(quantized_latents)\n",
        "    return keras.Model(inputs, reconstructions, name=\"rvq_vae\")\n",
        "\n",
        "get_vqvae().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwzLMWy6UUh3"
      },
      "source": [
        "## Wrapping up the training loop inside `RVQVAETrainer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QbfBDo8iUUh3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RVQVAETrainer(keras.models.Model):\n",
        "    def __init__(self, train_variance, latent_dim=32, num_embeddings=128, num_quantizers = 2,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.train_variance = train_variance\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.num_quantizers = num_quantizers\n",
        "\n",
        "        self.rvqvae = get_vqvae(self.latent_dim, self.num_embeddings, self.num_quantizers)\n",
        "\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.rvq_loss_tracker = keras.metrics.Mean(name=\"rvq_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.rvq_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Outputs from the VQ-VAE.\n",
        "            reconstructions = self.rvqvae(x)\n",
        "\n",
        "            # Calculate the losses.\n",
        "            reconstruction_loss = (\n",
        "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
        "            )\n",
        "            total_loss = reconstruction_loss + sum(self.rvqvae.losses)\n",
        "\n",
        "        # Backpropagation.\n",
        "        grads = tape.gradient(total_loss, self.rvqvae.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.rvqvae.trainable_variables))\n",
        "\n",
        "        # Loss tracking.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.rvq_loss_tracker.update_state(sum(self.rvqvae.losses))\n",
        "\n",
        "        # Log results.\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"rvqvae_loss\": self.rvq_loss_tracker.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5ljEADUUh4"
      },
      "source": [
        "## Load and preprocess the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST-R9nMNUUh4",
        "outputId": "bc130232-e750-4370-c910-7e9b9848418d"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "x_train_scaled = (x_train / 255.0) - 0.5\n",
        "x_test_scaled = (x_test / 255.0) - 0.5\n",
        "\n",
        "data_variance = np.var(x_train / 255.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C2aF4jXUUh4"
      },
      "source": [
        "## Train the VQ-VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5on6YX9UUh4",
        "outputId": "70b0de92-1b0b-4274-e3fb-fa90c602f212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 76ms/step - loss: 19.6317 - reconstruction_loss: 0.6243 - rvqvae_loss: 19.0074\n",
            "Epoch 2/3\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - loss: 61.7102 - reconstruction_loss: 0.1057 - rvqvae_loss: 61.6045\n",
            "Epoch 3/3\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - loss: 64.5933 - reconstruction_loss: 0.0899 - rvqvae_loss: 64.5035\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x2a84d8dbc90>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vqvae_trainer = RVQVAETrainer(data_variance, latent_dim=16, num_embeddings=128, num_quantizers =2)\n",
        "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
        "vqvae_trainer.fit(x_train_scaled, epochs=3, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NaPvlpeUUh5"
      },
      "source": [
        "## Reconstruction results on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-HsHZ60qUUh5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "def show_subplot(original, reconstructed):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original.squeeze() + 0.5)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(reconstructed.squeeze() + 0.5)\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "samples = 2\n",
        "trained_vqvae_model = vqvae_trainer.rvqvae\n",
        "idx = np.random.choice(len(x_test_scaled), 10)\n",
        "test_images = x_test_scaled[idx]\n",
        "test_images_sub = test_images[:samples]\n",
        "reconstructions_test = trained_vqvae_model.predict(test_images_sub)\n",
        "\n",
        "for test_image, reconstructed_image in zip(test_images_sub, reconstructions_test):\n",
        "    show_subplot(test_image, reconstructed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh3F-yIwUUh5"
      },
      "source": [
        "These results look decent. You are encouraged to play with different hyperparameters\n",
        "(especially the number of embeddings and the dimensions of the embeddings) and observe how\n",
        "they affect the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JBu8GgUUh5"
      },
      "source": [
        "## Visualizing the discrete codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "OYwGaSXqUUh5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Error at quantization stage 1 (L2 norm): 47.4276008605957\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the encoder and vector quantizer layers\n",
        "encoder = vqvae_trainer.rvqvae.get_layer(\"encoder\")\n",
        "quantizer = vqvae_trainer.rvqvae.get_layer(\"vector_quantizer\")\n",
        "\n",
        "#\n",
        "num_random_images = 1\n",
        "random_indices = np.random.choice(test_images.shape[0], num_random_images, replace=False)\n",
        "\n",
        "# Select the random images using the random indices\n",
        "test_images_subset = test_images[random_indices]\n",
        "\n",
        "\n",
        "encoded_outputs = encoder.predict(test_images_subset)  # Shape: (batch, height, width, channels)\n",
        "\n",
        "# Reshape for quantization (flatten spatial dimensions)\n",
        "batch_size, height, width, channels = encoded_outputs.shape\n",
        "flat_enc_outputs = encoded_outputs.reshape(batch_size * height * width, channels)\n",
        "\n",
        "# Initialize a list to store the quantized outputs after each stage\n",
        "all_quantized_outputs = []\n",
        "\n",
        "# Loop over each quantization stage\n",
        "residual = flat_enc_outputs\n",
        "for i in range(quantizer.num_quantizers):\n",
        "    embedding_matrix = quantizer.embeddings[i]  # Get the embedding matrix for this stage\n",
        "    \n",
        "    # Get the quantized indices for the current stage\n",
        "    codebook_indices = quantizer.get_code_indices(residual, embedding_matrix)\n",
        "    \n",
        "    # Compute the quantized output for the current stage\n",
        "    encodings = tf.one_hot(codebook_indices, quantizer.num_embeddings)\n",
        "    quantized = tf.matmul(encodings, embedding_matrix, transpose_b=True)\n",
        "    \n",
        "    # Store the quantized output after this stage\n",
        "    all_quantized_outputs.append(quantized.numpy().reshape(batch_size, height, width, channels))\n",
        "\n",
        "    # Compute the residual for the next stage\n",
        "    residual -= quantized\n",
        "\n",
        "    # Compute the L2 norm error between the encoder output and the quantized output\n",
        "    error = tf.norm(residual, axis=-1)  # L2 norm error across channels\n",
        "    error_value = tf.reduce_mean(error).numpy()  # Take the average error for this stage\n",
        "    print(f\"Error at quantization stage {i + 1} (L2 norm): {error_value}\")\n",
        "\n",
        "# Visualize individual channels from the quantized output\n",
        "for i in range(len(test_images_subset)):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the original image\n",
        "    plt.subplot(1, len(all_quantized_outputs) + 1, 1)\n",
        "    plt.imshow(test_images_subset[i].squeeze() + 0.5, cmap=\"gray\")\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Plot individual channels from the quantized output after each quantization stage\n",
        "    for j, quantized_output in enumerate(all_quantized_outputs):\n",
        "        plt.subplot(1, len(all_quantized_outputs) + 1, j + 2)\n",
        "        plt.imshow(quantized_output[i, :, :, 0], cmap=\"viridis\")  # Show the first channel\n",
        "        plt.title(f\"Quantized Stage {j + 1} - Channel 0\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgH4xNZsUUh6"
      },
      "source": [
        "The figure above shows that the discrete codes have been able to capture some\n",
        "regularities from the dataset. Now, how do we sample from this codebook to create\n",
        "novel images? Since these codes are discrete and we imposed a categorical distribution\n",
        "on them, we cannot use them yet to generate anything meaningful until we can generate likely\n",
        "sequences of codes that we can give to the decoder.\n",
        "\n",
        "The authors use a PixelCNN to train these codes so that they can be used as powerful priors to\n",
        "generate novel examples. PixelCNN was proposed in\n",
        "[Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)\n",
        "by van der Oord et al. We borrow the implementation from\n",
        "[this PixelCNN example](https://keras.io/examples/generative/pixelcnn/). It's an autoregressive\n",
        "generative model where the outputs are conditional on the prior ones. In other words, a PixelCNN\n",
        "generates an image on a pixel-by-pixel basis. For the purpose in this example, however, its task\n",
        "is to generate code book indices instead of pixels directly. The trained VQ-VAE decoder is used\n",
        "to map the indices generated by the PixelCNN back into the pixel space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtuqlg-AUUh6"
      },
      "source": [
        "## PixelCNN hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W-To9U9UUh6"
      },
      "outputs": [],
      "source": [
        "num_residual_blocks = 2\n",
        "num_pixelcnn_layers = 2\n",
        "pixelcnn_input_shape = encoded_outputs.shape[1:-1]\n",
        "print(f\"Input shape of the PixelCNN: {pixelcnn_input_shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZFHS_8hUUh6"
      },
      "source": [
        "This input shape represents the reduction in the resolution performed by the encoder. With \"same\" padding,\n",
        "this exactly halves the \"resolution\" of the output shape for each stride-2 convolution layer. So, with these\n",
        "two layers, we end up with an encoder output tensor of 7x7 on axes 2 and 3, with the first axis as the batch\n",
        "size and the last axis being the code book embedding size. Since the quantization layer in the autoencoder\n",
        "maps these 7x7 tensors to indices of the code book, these output layer axis sizes must be matched by the\n",
        "PixelCNN as the input shape. The task of the PixelCNN for this architecture is to generate _likely_ 7x7\n",
        "arrangements of codebook indices.\n",
        "\n",
        "Note that this shape is something to optimize for in larger-sized image domains, along with the code\n",
        "book sizes. Since the PixelCNN is autoregressive, it needs to pass over each codebook index sequentially\n",
        "in order to generate novel images from the codebook. Each stride-2 (or rather more correctly a\n",
        "stride (2, 2)) convolution layer will divide the image generation time by four. Note, however, that there\n",
        "is probably a lower bound on this part: when the number of codes for the image to reconstruct is too small,\n",
        "it has insufficient information for the decoder to represent the level of detail in the image, so the\n",
        "output quality will suffer. This can be amended at least to some extent by using a larger code book.\n",
        "Since the autoregressive part of the image generation procedure uses codebook indices, there is far less of\n",
        "a performance penalty on using a larger code book as the lookup time for a larger-sized code from a larger\n",
        "code book is much smaller in comparison to iterating over a larger sequence of code book indices, although\n",
        "the size of the code book does impact on the batch size that can pass through the image generation procedure.\n",
        "Finding the sweet spot for this trade-off can require some architecture tweaking and could very well differ\n",
        "per dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmGoin7xUUh6"
      },
      "source": [
        "## PixelCNN model\n",
        "\n",
        "Majority of this comes from\n",
        "[this example](https://keras.io/examples/generative/pixelcnn/).\n",
        "\n",
        "## Notes\n",
        "Thanks to [Rein van 't Veer](https://github.com/reinvantveer) for improving this example with\n",
        "copy-edits and minor code clean-ups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFr5XEe1UUh6"
      },
      "outputs": [],
      "source": [
        "# The first layer is the PixelCNN layer. This layer simply\n",
        "# builds on the 2D convolutional layer, but includes masking.\n",
        "class PixelConvLayer(layers.Layer):\n",
        "    def __init__(self, mask_type, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mask_type = mask_type\n",
        "        self.conv = layers.Conv2D(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Build the conv2d layer to initialize kernel variables\n",
        "        self.conv.build(input_shape)\n",
        "        # Use the initialized kernel to create the mask\n",
        "        kernel_shape = self.conv.kernel.get_shape()\n",
        "        self.mask = np.zeros(shape=kernel_shape)\n",
        "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
        "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
        "        if self.mask_type == \"B\":\n",
        "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
        "        return self.conv(inputs)\n",
        "\n",
        "\n",
        "# Next, we build our residual block layer.\n",
        "# This is just a normal residual block, but based on the PixelConvLayer.\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.conv1 = keras.layers.Conv2D(\n",
        "            filters=filters, kernel_size=1, activation=\"relu\"\n",
        "        )\n",
        "        self.pixel_conv = PixelConvLayer(\n",
        "            mask_type=\"B\",\n",
        "            filters=filters // 2,\n",
        "            kernel_size=3,\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "        )\n",
        "        self.conv2 = keras.layers.Conv2D(\n",
        "            filters=filters, kernel_size=1, activation=\"relu\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pixel_conv(x)\n",
        "        x = self.conv2(x)\n",
        "        return keras.layers.add([inputs, x])\n",
        "\n",
        "\n",
        "pixelcnn_inputs = keras.Input(shape=pixelcnn_input_shape, dtype=tf.int32)\n",
        "ohe = tf.one_hot(pixelcnn_inputs, vqvae_trainer.num_embeddings)\n",
        "x = PixelConvLayer(\n",
        "    mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\"\n",
        ")(ohe)\n",
        "\n",
        "for _ in range(num_residual_blocks):\n",
        "    x = ResidualBlock(filters=128)(x)\n",
        "\n",
        "for _ in range(num_pixelcnn_layers):\n",
        "    x = PixelConvLayer(\n",
        "        mask_type=\"B\",\n",
        "        filters=128,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        activation=\"relu\",\n",
        "        padding=\"valid\",\n",
        "    )(x)\n",
        "\n",
        "out = keras.layers.Conv2D(\n",
        "    filters=vqvae_trainer.num_embeddings, kernel_size=1, strides=1, padding=\"valid\"\n",
        ")(x)\n",
        "\n",
        "pixel_cnn = keras.Model(pixelcnn_inputs, out, name=\"pixel_cnn\")\n",
        "pixel_cnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms9lnXHbUUh6"
      },
      "source": [
        "## Prepare data to train the PixelCNN\n",
        "\n",
        "We will train the PixelCNN to learn a categorical distribution of the discrete codes.\n",
        "First, we will generate code indices using the encoder and vector quantizer we just\n",
        "trained. Our training objective will be to minimize the crossentropy loss between these\n",
        "indices and the PixelCNN outputs. Here, the number of categories is equal to the number\n",
        "of embeddings present in our codebook (128 in our case). The PixelCNN model is\n",
        "trained to learn a distribution (as opposed to minimizing the L1/L2 loss), which is where\n",
        "it gets its generative capabilities from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bt0Z-lXUUh6"
      },
      "outputs": [],
      "source": [
        "# Generate the codebook indices.\n",
        "encoded_outputs = encoder.predict(x_train_scaled)\n",
        "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
        "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
        "\n",
        "codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
        "print(f\"Shape of the training data for PixelCNN: {codebook_indices.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OeLNaiyUUh7"
      },
      "source": [
        "## PixelCNN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C9K15ZyUUh7"
      },
      "outputs": [],
      "source": [
        "pixel_cnn.compile(\n",
        "    optimizer=keras.optimizers.Adam(3e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "pixel_cnn.fit(\n",
        "    x=codebook_indices,\n",
        "    y=codebook_indices,\n",
        "    batch_size=128,\n",
        "    epochs=30,\n",
        "    validation_split=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUqlCGhXUUh7"
      },
      "source": [
        "We can improve these scores with more training and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3p-FDj_UUh7"
      },
      "source": [
        "## Codebook sampling\n",
        "\n",
        "Now that our PixelCNN is trained, we can sample distinct codes from its outputs and pass\n",
        "them to our decoder to generate novel images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6csp1zDBUUh7"
      },
      "outputs": [],
      "source": [
        "# Create a mini sampler model.\n",
        "inputs = layers.Input(shape=pixel_cnn.input_shape[1:])\n",
        "outputs = pixel_cnn(inputs, training=False)\n",
        "categorical_layer = tfp.layers.DistributionLambda(tfp.distributions.Categorical)\n",
        "outputs = categorical_layer(outputs)\n",
        "sampler = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCQQ2Uo8UUh7"
      },
      "source": [
        "We now construct a prior to generate images. Here, we will generate 10 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2sxjkVWUUh7"
      },
      "outputs": [],
      "source": [
        "# Create an empty array of priors.\n",
        "batch = 10\n",
        "priors = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
        "batch, rows, cols = priors.shape\n",
        "\n",
        "# Iterate over the priors because generation has to be done sequentially pixel by pixel.\n",
        "for row in range(rows):\n",
        "    for col in range(cols):\n",
        "        # Feed the whole array and retrieving the pixel value probabilities for the next\n",
        "        # pixel.\n",
        "        probs = sampler.predict(priors)\n",
        "        # Use the probabilities to pick pixel values and append the values to the priors.\n",
        "        priors[:, row, col] = probs[:, row, col]\n",
        "\n",
        "print(f\"Prior shape: {priors.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV07VNuKUUiJ"
      },
      "source": [
        "We can now use our decoder to generate the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJz1ajVVUUiJ"
      },
      "outputs": [],
      "source": [
        "# Perform an embedding lookup.\n",
        "pretrained_embeddings = quantizer.embeddings\n",
        "priors_ohe = tf.one_hot(priors.astype(\"int32\"), vqvae_trainer.num_embeddings).numpy()\n",
        "quantized = tf.matmul(\n",
        "    priors_ohe.astype(\"float32\"), pretrained_embeddings, transpose_b=True\n",
        ")\n",
        "quantized = tf.reshape(quantized, (-1, *(encoded_outputs.shape[1:])))\n",
        "\n",
        "# Generate novel images.\n",
        "decoder = vqvae_trainer.vqvae.get_layer(\"decoder\")\n",
        "generated_samples = decoder.predict(quantized)\n",
        "\n",
        "for i in range(batch):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(priors[i])\n",
        "    plt.title(\"Code\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(generated_samples[i].squeeze() + 0.5)\n",
        "    plt.title(\"Generated Sample\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3dKOwOUUUiJ"
      },
      "source": [
        "We can enhance the quality of these generated samples by tweaking the PixelCNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFx3wFOtUUiJ"
      },
      "source": [
        "## Additional notes\n",
        "\n",
        "* After the VQ-VAE paper was initially released, the authors developed an exponential\n",
        "moving averaging scheme to update the embeddings inside the quantizer. If you're\n",
        "interested you can check out\n",
        "[this snippet](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py#L124).\n",
        "* To further enhance the quality of the generated samples,\n",
        "[VQ-VAE-2](https://arxiv.org/abs/1906.00446) was proposed that follows a cascaded\n",
        "approach to learn the codebook and to generate the images."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
